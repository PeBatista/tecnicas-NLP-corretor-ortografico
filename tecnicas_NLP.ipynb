{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lZ42zE93XT0"
      },
      "outputs": [],
      "source": [
        "# lendo aruquivos com OPEN comando python, with as f (file)\n",
        "with open('artigos.txt', \"r\") as f:\n",
        "  artigos = f.read()\n",
        "print(artigos[:500]) # limitando os primeiros 500 caracteres"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# analisando estes dados dentro de arquivos"
      ],
      "metadata": {
        "id": "GaCS7bbk73z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# como saber quantas palavras, temos dentro de uma variável String, sabendo que é separada por espaço.\n",
        "# o split serveria para colocar está string inteira dentro de um array, separando cada palavras.\n",
        "# temos tokens separados, por causa das pontuações ainda existentes. \n",
        "# mas de antimão, conseguimos assim, poder ver quantas palavras temos.\n",
        "teste = 'Olá, tudo bem?'\n",
        "tokens = teste.split()\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "kfm7DtFO78xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processo de refinamento do token para palavra, neste caso, instalamos o NLTK - para tratar disto\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "palavras_separadas = nltk.tokenize.word_tokenize(teste) # função de separar, as palavras e pontuações.\n",
        "palavras_separadas"
      ],
      "metadata": {
        "id": "9KYJvCNfC9WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# função de tirar as pontuações\n",
        "def separa_palavra(lista_token):\n",
        "  lista_palavras = []\n",
        "  for token in lista_token:\n",
        "\n",
        "    if token.isalpha():  # método que percorre a string, e pergunta se só tem caractereres alfabéticos?\n",
        "      lista_palavras.append(token) \n",
        "    \n",
        "  return lista_palavras\n"
      ],
      "metadata": {
        "id": "E2pN36y6E7qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_artigo = nltk.tokenize.word_tokenize(artigos)\n",
        "palavras_artigo = separa_palavra(tokens_artigo)\n",
        "len(palavras_artigo) # quantidade de paralvras, dentro de nosso artigo."
      ],
      "metadata": {
        "id": "Xu8k554KYSbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NORMALIZANDO\n",
        "# transformando a minha lista em apenas, minúsculas.\n",
        "\n",
        "def normalizacao(lista_palavras):\n",
        "  lista_normalizada = []\n",
        "  for palavra in lista_palavras:\n",
        "    lista_normalizada.append(palavra.lower())\n",
        "  return lista_normalizada\n",
        "palavras_artigo_repetidos = normalizacao(palavras_artigo)\n",
        "\n",
        "len(palavras_artigo_repetidos) # assim teremos está mesma, normalizada.\n",
        "\n",
        "# removendo as palavras repetidas\n",
        "# SET, idea de conjuntos, naturais e etc. REPARE QUE NÃO TEMOS REPETIÇÕES. Então pegaremos uma lista de números, e retornará o conjt.\n",
        "palavras_artigo = set(palavras_artigo_repetidos) # número de tipos de palavras... dentro de nossa lista de palavras do artigo da Alura\n",
        "len(palavras_artigo)"
      ],
      "metadata": {
        "id": "WqitG2hLZ_Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var = 'plavra'\n",
        "(var[:1],var[1:]) # tupla de representação lado direito e esquerdo, a partir da primeira posição."
      ],
      "metadata": {
        "id": "N5zBaJFOlLz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def insere_letras(fatias): # recebe as fatias das palavras, e retorna as novas palavras, a partir de uma.\n",
        "  novas_palavras = []\n",
        "  letras = 'abcdefghijklmnopqrstuvwxyzáâàãéêèẽíîìĩóôõòúûùũç'\n",
        "  \n",
        "  for E, D in fatias: # já que é uma tupla, ele entederá normalmente\n",
        "    for letra in letras:\n",
        "      novas_palavras.append(E + letra + D)\n",
        "\n",
        "  return novas_palavras\n"
      ],
      "metadata": {
        "id": "lo4kva2Ak4Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# função de deletar letras para o seguinte erro (palavvra)\n",
        "\n",
        "\n",
        "def deleta_letras(fatias): # recebe as fatias das palavras, e retorna as novas palavras, a partir de uma.\n",
        "  novas_palavras = []\n",
        "  \n",
        "  for E, D in fatias: # já que é uma tupla, ele entederá normalmente\n",
        "    novas_palavras.append(E + D[1:])\n",
        "\n",
        "  return novas_palavras\n"
      ],
      "metadata": {
        "id": "DG0nT7HO7ihH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# função de trocar as letras para o seguinte erro (palafra)\n",
        "\n",
        "\n",
        "def troca_letras(fatias): # recebe as fatias das palavras, e retorna as novas palavras, a partir de uma.\n",
        "  novas_palavras = []\n",
        "  letras = 'abcdefghijklmnopqrstuvwxyzáâàãéêèẽíîìĩóôõòúûùũç'\n",
        "\n",
        "  for E, D in fatias: # já que é uma tupla, ele entederá normalmente\n",
        "    for letra in letras:\n",
        "      novas_palavras.append(E + letra + D[1:])\n",
        "\n",
        "  return novas_palavras\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "9UZ83oP0_C_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# função de inveter as letras para o seguinte erro (palarva)\n",
        "\n",
        "\n",
        "def inverte_letras(fatias): # recebe as fatias das palavras, e retorna as novas palavras, a partir de uma.\n",
        "  novas_palavras = []\n",
        "\n",
        "  for E, D in fatias: # já que é uma tupla, ele entederá normalmente\n",
        "    if len(D) > 1: # condição se D tiver o tamanho de contagem de letras maior que 1\n",
        "      novas_palavras.append(E + D[1]+ D[0] + D[2:])\n",
        "\n",
        "  return novas_palavras\n",
        "\n",
        "  \n",
        "\n",
        "def gerador_palavras(palavra):\n",
        "  novas_palavras = []\n",
        "  fatias = []\n",
        "  for i in range(len(palavra) + 1): # i = index da palavra, + 1 em cima do tamanho para percorrer toda a palavra mesmo = ('lgica','')\n",
        "    fatias.append((palavra[:i],palavra[i:])) # guardando cada fateamento\n",
        "  novas_palavras = insere_letras(fatias) \n",
        "  novas_palavras += deleta_letras(fatias) \n",
        "  novas_palavras += troca_letras(fatias)\n",
        "  novas_palavras += inverte_letras(fatias)\n",
        "  return novas_palavras\n",
        "\n",
        "\n",
        "palavra_exemplo_errada = 'lóigica'\n",
        "palavras_geradas = gerador_palavras(palavra_exemplo_errada)\n",
        "# 'lógica' in palavras_geradas verificar se temos a palavra correta, a partir de lóigica\n"
      ],
      "metadata": {
        "id": "r5QPNt6LDoOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frequência da palavra sobre o total de palavra -- quantidade de vezes que ela aparece dentro de nosso corpos \n",
        "frequencia = nltk.FreqDist(palavras_artigo_repetidos) # calcula a distribuição de frequência de nossas palavras normalizadas, com repetição e etc, para ver quantas vezes \"lógica existe\"\n",
        "frequencia # um dicionário, relacionado com as palavras e quantidade de vezes que mostram a mesma.\n",
        "total_de_palavras = len(palavras_artigo_repetidos) # total_de_palavras com as repetições dela dentro do artigo\n",
        "\n",
        "\n",
        "# função que vai ver das palavras geradas a lógica é a correta\n",
        "def probabilidade(palavra_gerada):\n",
        "  return frequencia[palavra_gerada]/total_de_palavras\n",
        "\n",
        "def corretor(palavra):\n",
        "  palavras_geradas = gerador_palavras(palavra)\n",
        "  palavra_correta = max(palavras_geradas, key=probabilidade) # PROBABILIDADE MÁXIMA DE SER IGUAL A ESTAS GERADAS, para cada uma, e retornar a correta.\n",
        "  return palavra_correta\n",
        "\n",
        "# a gente corrige, somente um tipo de erro, quando sem querer esquecemos alguma letra.\n",
        "\n",
        "palavra_correta = corretor('computadro')\n",
        "palavra_correta\n"
      ],
      "metadata": {
        "id": "YeoZGf8JDbtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# precisamos fazer o pc entender, se a palavra corrigida é a mesma palavra correta, TESTE\n",
        "\n",
        "def cria_dados_teste(base_dados_teste): \n",
        "  # ler cada uma dqls linhas e separar o lado correto do lado incorreto\n",
        "  lista_palavras_teste = []\n",
        "  f = open(base_dados_teste,'r')\n",
        "  for linha in f:\n",
        "    correta, errada = linha.split()\n",
        "    lista_palavras_teste.append((correta,errada))\n",
        "  f.close()\n",
        "  return lista_palavras_teste\n",
        "\n",
        "lista_teste = cria_dados_teste('palavras.txt') \n",
        "# temos a base de dados na estrutura fácil de criar nosso avaliador\n",
        "\n",
        "\n",
        "\n",
        "def avaliador(dados_teste, vocabulario):\n",
        "  numero_palavras = len(dados_teste) # qtd de palavras na nossa base de testes\n",
        "  acertou = 0\n",
        "  desconhecido = 0\n",
        "  # contador de acertos, implementado cada vez que nosso corretor acertar as palavras, sendo a mesma\n",
        "  for correta, errada in dados_teste:\n",
        "    palavra_corrigida = corretor(errada) # testando se corrige bem, tem que ser igual a do lado Escorreta\n",
        "    desconhecido += (correta not in vocabulario) # desconhecida incrementada quando, a palavra correta not in vocabulario\n",
        "    if palavra_corrigida == correta:\n",
        "      acertou += 1 # adiciono 1 a minha variável acertou\n",
        "  taxa_acerto_porcento = acertou/numero_palavras\n",
        "  taxa_desconhecida =  desconhecido/numero_palavras\n",
        "\n",
        "\n",
        "  print(f'taxa de acerto: {round(taxa_acerto_porcento*100,2)}% de {numero_palavras} palavras testadas ')\n",
        "  print(f'e taxa de desconhecido: {round(taxa_desconhecida*100,2)}% em {numero_palavras} palavras testadas')\n",
        "\n",
        "avaliador(lista_teste, palavras_artigo) \n",
        "\n",
        "# taxa de acerto: 76.34% de 186 palavras testadas \n",
        "# e taxa de desconhecido: 6.99% em 186 palavras testadas"
      ],
      "metadata": {
        "id": "KfmFJVUZTKrv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}